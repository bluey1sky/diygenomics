{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426ce1e9-075b-4c6b-b8df-b18b516d0912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import markdown\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fbb85-04bd-45cf-8e9e-7deecb94e7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_name = \"2016_Guo_AdS_Numerics\"\n",
    "project_folder = \"diygenomics-projects\"\n",
    "sub_category = \"math\"\n",
    "work_bucket = \"AdS-CFT\"\n",
    "external_id = \"2023_06_01_e4e21f7112268ec22c17g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06aaeb2-40d2-4361-8f9f-f3e0731bf07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'http://localhost:999/create'\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "max_chunk_size = 16000\n",
    "overlap_size = 10\n",
    "# headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "data_path = os.getenv('DATA_PATH')\n",
    "file_path = lambda *args: os.path.join(data_path, project_folder, sub_category, work_bucket, base_name, 'mathpix', *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cf39d1-f3fa-49cb-b723-ce2a4b6573cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_path = file_path(f'{external_id}.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1f5252-cd6d-445f-bec9-51ddadade228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(document_path, 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ee0b08-d90e-437c-9c2a-98950130df20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('system_create.txt', 'r') as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84ea6d19-5e12-41be-936a-28bf196c456e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def split_conversation(conversation, system_prompt, token_limit):\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#     tokens = tokenizer.encode(conversation)\n",
    "#     prompt_tokens = tokenizer.encode(system_prompt)\n",
    "#     max_tokens = token_limit - len(prompt_tokens)\n",
    "\n",
    "#     # Calculate the number of chunks we need to split the tokens into\n",
    "#     num_chunks = math.ceil(len(tokens) / max_tokens)\n",
    "\n",
    "#     # Split the tokens into chunks\n",
    "#     token_chunks = [tokens[i * max_tokens:(i + 1) * max_tokens] for i in range(num_chunks)]\n",
    "\n",
    "#     # Decode each chunk back into text\n",
    "#     text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]\n",
    "\n",
    "#     return text_chunks\n",
    "def split_conversation(conversation, system_prompt, token_limit):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokens = tokenizer.encode(conversation)\n",
    "    prompt_tokens = tokenizer.encode(system_prompt)\n",
    "    max_tokens = token_limit - len(prompt_tokens)\n",
    "    \n",
    "    # Split the tokens into chunks\n",
    "    token_chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        end = min(i + max_tokens, len(tokens))\n",
    "        \n",
    "        # Check if the chunk ends in the middle of a LaTeX block\n",
    "        chunk = tokens[i:end]\n",
    "        chunk_text = tokenizer.decode(chunk)\n",
    "        if chunk_text.count('$$') % 2 != 0:  # LaTeX block is not complete\n",
    "            # Find the start of the incomplete LaTeX block\n",
    "            latex_start = chunk_text.rfind('$$')\n",
    "            # Adjust the end of the chunk to be before the LaTeX block\n",
    "            end = i + len(tokenizer.encode(chunk_text[:latex_start]))\n",
    "        token_chunks.append(tokens[i:end])\n",
    "        i = end\n",
    "\n",
    "    # Decode each chunk back into text\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c8b107f-a93a-4050-a04d-6f7764bea4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18729 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "text_chunks = split_conversation(text, system_prompt, 16384)\n",
    "\n",
    "for index, text_chunk in enumerate(text_chunks):\n",
    "    with open(f'{base_name}_{index}.txt', 'w') as f:\n",
    "        f.write(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8021016-0db9-4c76-80fb-50047dc7f62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_kb_document(text_chunk):\n",
    "    text_chunk = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text_chunk)\n",
    "\n",
    "    payload = {\n",
    "        'input': text_chunk\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    print('Status code:', response.status_code)\n",
    "    print('Response:', response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cedc613-f347-4396-bdd9-b0840dc77e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create_kb_document('estasfaf asdfasf test test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fefb28c2-6828-41f1-b533-f467e13fadda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, text_chunk in enumerate(text_chunks):\n",
    "    with open(f'{base_name}_{index}.txt', 'w') as f:\n",
    "        f.write(text_chunk)\n",
    "    # create_kb_document(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd5ea6-4073-48c7-a9c6-172608f4d3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
